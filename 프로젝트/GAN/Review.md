<aside>
ğŸ¤” í”¼ì–´ë¦¬ë·° í…œí”Œë¦¿

- [o]  **1. ì£¼ì–´ì§„ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œê°€ ì œì¶œë˜ì—ˆë‚˜ìš”? (ì™„ì„±ë„)**
    - ë¬¸ì œì—ì„œ ìš”êµ¬í•˜ëŠ” ìµœì¢… ê²°ê³¼ë¬¼ì´ ì²¨ë¶€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    - ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œë€ í”„ë¡œì íŠ¸ ë£¨ë¸Œë¦­ 3ê°œ ì¤‘ 2ê°œ, 
    í€˜ìŠ¤íŠ¸ ë¬¸ì œ ìš”êµ¬ì¡°ê±´ ë“±ì„ ì§€ì¹­
        - í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¶€ë¶„ì˜ ì½”ë“œ ë° ê²°ê³¼ë¬¼ì„ ìº¡ì³í•˜ì—¬ ì‚¬ì§„ìœ¼ë¡œ ì²¨ë¶€

- [o]  **2. í”„ë¡œì íŠ¸ì—ì„œ í•µì‹¬ì ì¸ ë¶€ë¶„ì— ëŒ€í•œ ì„¤ëª…ì´ ì£¼ì„(ë‹¥ìŠ¤íŠ¸ë§) ë° ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ì˜ ê¸°ë¡ë˜ì–´ìˆë‚˜ìš”? (ì„¤ëª…)**
    - [o]  ëª¨ë¸ ì„ ì • ì´ìœ 
    - [o]  í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„ ì • ì´ìœ 
    - [o]  ë°ì´í„° ì „ì²˜ë¦¬ ì´ìœ  ë˜ëŠ” ë°©ë²• ì„¤ëª…

- [o]  **3. ì²´í¬ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” í•­ëª©ë“¤ì„ ìˆ˜í–‰í•˜ì˜€ë‚˜ìš”? (ë¬¸ì œ í•´ê²°)**
    - [o]  ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆë‚˜ìš”? (train, validation, test ë°ì´í„°ë¡œ êµ¬ë¶„)
    - [o]  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•´ê°€ë©° ì—¬ëŸ¬ ì‹œë„ë¥¼ í–ˆë‚˜ìš”? (learning rate, dropout rate, unit, batch size, epoch ë“±)
    - [o]  ê° ì‹¤í—˜ì„ ì‹œê°í™”í•˜ì—¬ ë¹„êµí•˜ì˜€ë‚˜ìš”?
    - [o]  ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ê°€ ê¸°ë¡ë˜ì—ˆë‚˜ìš”?

- [o]  **4. í”„ë¡œì íŠ¸ì— ëŒ€í•œ íšŒê³ ê°€ ìƒì„¸íˆ ê¸°ë¡ ë˜ì–´ ìˆë‚˜ìš”? (íšŒê³ , ì •ë¦¬)**
    - [o]  ë°°ìš´ ì 
    - [o]  ì•„ì‰¬ìš´ ì 
    - [o]  ëŠë‚€ ì 
    - [o]  ì–´ë ¤ì› ë˜ ì 

- [o]  **5.  ì•±ìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ë‚˜ìš”?**
    - [o]  êµ¬í˜„ëœ ì•±ì´ ì˜ ë™ì‘í•œë‹¤.
    - [o]  ëª¨ë¸ì´ ì˜ ë™ì‘í•œë‹¤.
</aside>

ë¯¸ì„¸ì¡°ì •ëœ ë‘ ê°€ì§€ ëª¨ë¸ê³¼ ì•± í•˜ë‚˜ë¥¼ êµ¬í˜„í•´ì£¼ì…¨ìœ¼ë©° GIF ì´ë¯¸ì§€ë„ ì²¨ë¶€í•´ì£¼ì…¨ë‹¤.
ëª¨ë¸ ì„±ëŠ¥ì€ ê°œì„ ì¤‘ì´ì‹œë©° ì•±ì€ ëª¨ë¸ê³¼ ì˜ ì—°ë™ë˜ì–´ ì„±ê³µì ìœ¼ë¡œ ê²°ê³¼ê°’ì„ ì¶œë ¥í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. (ì¶œë ¥ê°’ì˜ ì •í™•ì„±ì€ ëª¨ë¸ ê°œì„ ì„ í†µí•´ ì´ë£¨ì–´ì§ˆ ì˜ˆì •)

ëª¨ë¸ í•˜ë‚˜(ìƒìš´ë‹˜):
* Denseë¥¼ ë” ì¶”ê°€
* Flattenì€ ì—°ì‚°ëŸ‰ì´ ëŠšìœ¼ë¡œ global average poolingìœ¼ë¡œ ë³€ê²½ (ìœ„ì¹˜ ì •ë³´ ìœ ì§€)
ê¸°ì¡´ ëª¨ë¸ì—ì„œ ì„±ëŠ¥ê³¼ ì—°ì‚°ëŸ‰ì„ ê°œì„ í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ êµ¬í˜„í•´ì£¼ì…¨ê³ , í•™ìŠµì‹œì— í‘ë°±ìœ¼ë¡œ í•˜ë©´ ì—°ì‚°ëŸ‰ì´ ì¤„ì§€ ì§ˆë¬¸ì„ ê³µìœ í•˜ë©° ì—°ì‚°ëŸ‰ì— ëŒ€í•´ ë‚˜ì•„ê°€ íƒêµ¬í•´ì£¼ì…¨ë‹¤.

ëª¨ë¸ ë‘˜(ê¸°í™ë‹˜):
* ë…¸ì´ì¦ˆì¸í’‹ê³¼ ë”ë¶ˆì–´ ë¼ë²¨ì¸í’‹ì„ í•¨ê»˜ í•™ìŠµì‹œì¼œ 110ì°¨ì›ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰
* í´ë˜ìŠ¤ ì„ íƒí•´ì„œ ì‚¬ì§„ì„ ë³¼ ìˆ˜ ìˆëŠ” ì•±ìœ¼ë¡œ êµ¬í˜„ (ë¼ë²¨ê³¼ ì´ë¯¸ì§€ê°€ ì •í™•íˆ ì—°ê²°ë˜ëŠ”ì§€ëŠ” í™•ì¸ í•„ìš”í•˜ì‹  ìƒíƒœ)
* ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ êµ¬ì„±í•´ ì¶”ê°€
ì•±ê³¼ ì—°ë™í•˜ëŠ” ì‹œë„ì™€, ë°°ì› ë˜ ì–´í…ì…˜ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì…¨ê³ , í›ˆë ¨ ì‹œê°„ì´ 50ì—í¬í¬ì— 1ì‹œê°„ë°˜ì´ ê±¸ë¦°ë‹¤ëŠ” ì ì„ ê³µìœ í•´ì£¼ì‹œë©° ì–´í…ì…˜ ë ˆì´ì–´ì˜ ì—°ì‚°ëŸ‰ì„ ë‹¤ì‹œ ê°€ëŠ í•´ì£¼ì…¨ë‹¤.

```
# ëª¨ë¸ í•˜ë‚˜: Dense ì¸µì´ ì¶”ê°€ë˜ê³  Flatten ëŒ€ì‹  Global Average Pooling ì¸µì´ ì ìš©ëœ ëª¨ë¸
def make_discriminator_model():
    model = tf.keras.Sequential()
    ...
    model.add(layers.Dropout(0.3))

    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(256))
    model.add(layers.Dense(128))
    model.add(layers.Dense(1))

    return model

```

```
# ëª¨ë¸ ë‘˜: ë¼ë²¨ í•™ìŠµ
def make_generator_model():
    noise_input = layers.Input(shape=(100,))
    label_input = layers.Input(shape=(10,))
    
    x = layers.Concatenate()([noise_input, label_input])
...
    model = tf.keras.Model([noise_input, label_input], x)
    return model

```

```
# ëª¨ë¸ ë‘˜: ë¼ë²¨ ì¸í’‹ ì—°ê²°
def train_step(images, labels):
    ...
    
    # ì›-í•« ì¸ì½”ë”©ëœ ë ˆì´ë¸” ìƒì„±
    one_hot_labels = tf.one_hot(tf.squeeze(labels), depth=10)

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator([noise, one_hot_labels], training=True)

        # ë ˆì´ë¸”ì„ ì´ë¯¸ì§€ì™€ ê²°í•©
        label_channels = tf.repeat(tf.expand_dims(one_hot_labels, axis=1), repeats=32, axis=1)
        label_channels = tf.repeat(tf.expand_dims(label_channels, axis=2), repeats=32, axis=2)
        
        real_input = tf.concat([images, label_channels], axis=3)
        fake_input = tf.concat([generated_images, label_channels], axis=3)
    ...
    return gen_loss, disc_loss
```

```
# ëª¨ë¸ ë‘˜: ì–´í…ì…˜ ë ˆì´ì–´
class SelfAttention(tf.keras.layers.Layer):
    def __init__(self, channels):
        super(SelfAttention, self).__init__()
        self.channels = channels
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=channels)
        self.ln = tf.keras.layers.LayerNormalization()
        self.ff_conv1 = tf.keras.layers.Conv2D(channels, 1, activation='relu')
        self.ff_conv2 = tf.keras.layers.Conv2D(channels, 1)

    def call(self, x):
        ...
        return x + output

    def compute_output_shape(self, input_shape):
        return input_shape
```

```
# ëª¨ë¸ ë‘˜: ì–´í…ì…˜ ë ˆì´ì–´ ì ìš©
# Generator ëª¨ë¸ ì •ì˜
def make_generator_model():
    model = tf.keras.Sequential()
    ...
    model.add(layers.Reshape((8, 8, 512)))
    ...
    model.add(layers.LeakyReLU())

    # Add Self-Attention layer
    model.add(SelfAttention(256))

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    ...
    return model

# Discriminator ëª¨ë¸ ì •ì˜
def make_discriminator_model():
    model = tf.keras.Sequential()
    ...
    model.add(layers.Dropout(0.3))
    
    # Add Self-Attention layer
    model.add(SelfAttention(128))
    
    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))
    ...
    
    return model

```
